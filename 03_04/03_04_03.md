# 3.4.3 Data Preprocessing - Encoding Schemes

## a. One hot
- One hot encoding is a method of encoding categorical variables for use in machine learning models. The purpose of one hot encoding is to represent each category as a binary vector, with a "1" in the position corresponding to the category and "0" in all other positions.
- One hot encoding is useful in machine learning because many algorithms cannot handle categorical data directly. Instead, they expect numerical input features. One hot encoding allows categorical data to be represented in a numerical form that can be used by these algorithms.
- For example, consider a dataset with a categorical feature "color" that has three categories: "red", "green", and "blue". Using one hot encoding, the "color" feature could be transformed into three binary features, "color_red", "color_green", and "color_blue", with a "1" in the corresponding position and "0" in all other positions.
- One hot encoding is a widely used method for encoding categorical variables, and is particularly useful for handling categorical variables with many categories, or for categorical variables that have an ordinal relationship. However, it can also increase the dimensionality of the data, which can be a problem in some cases.
- ADDITIONAL NOTES: 
    - Example: using 0,1,2 for red, blue, yellow implies a numeric relationship (yellow as 2 is twice as something as blue 1)

## b. Binarization
- Binarization is the process of thresholding a continuous-valued feature to create a binary (0/1) feature. It is a method of encoding features for use in machine learning models.
- The purpose of binarization is to transform continuous-valued features into a form that can be used by algorithms that can only handle binary data. It can also be used as a preprocessing step to convert continuous-valued features into a more suitable form for certain types of models, such as decision trees.
- To perform binarization, a threshold value is chosen, and all values above the threshold are encoded as "1", while all values below the threshold are encoded as "0". The threshold value can be chosen based on the characteristics of the data, or by using a method such as cross-validation to select the optimal threshold.
- Binarization can be useful in certain situations, such as when the continuous-valued feature has a clear separation at the threshold, or when the feature is not meaningful except in the presence or absence of a certain value. However, it can also be problematic if it leads to the loss of important information from the continuous-valued feature.
- ADDITIONAL NOTES:
    - Example: pass/fail a test

## c. Discretization
- Discretization is the process of dividing a continuous-valued feature into a set of discrete bins or intervals. It is a method of encoding features for use in machine learning models.
- The purpose of discretization is to transform continuous-valued features into a form that can be used by algorithms that can only handle categorical data. It can also be used as a preprocessing step to convert continuous-valued features into a more suitable form for certain types of models, such as decision trees.
- To perform discretization, the range of the continuous-valued feature is divided into a set of bins or intervals, and each value is assigned to the appropriate bin. The bins can be chosen based on the characteristics of the data, or by using a method such as equal-width binning or equal-frequency binning.
- Discretization can be useful in certain situations, such as when the continuous-valued feature has a clear separation at the bin boundaries, or when the feature is not meaningful except within certain ranges. However, it can also be problematic if it leads to the loss of important information from the continuous-valued feature.
- ADDITIONAL NOTES: 
    - Example, test grades to A/B/C/D/F.