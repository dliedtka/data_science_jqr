# Data Preprocessing - Unsupervised Learning

## a. Clustering
### K-means:
- K-means clustering is an unsupervised learning algorithm that is used to group data points into clusters based on their similarity. The goal of K-means clustering is to divide the data into K clusters, where each cluster is represented by the mean (centroid) of the data points in the cluster.
- To implement K-means clustering, the following steps can be followed:
    1. Select the number of clusters, K.
    2. Initialize K centroids randomly.
    3. Assign each data point to the cluster with the nearest centroid.
    4. Recompute the centroids of the clusters by taking the mean of all the data points in the cluster.
    5. Repeat steps 3 and 4 until the centroids do not change, or until a maximum number of iterations has been reached.
- One important aspect of K-means clustering is the initialization of the centroids. If the centroids are initialized randomly, it is possible for the algorithm to get stuck in a local minimum, resulting in poor cluster assignments. To avoid this problem, it is often recommended to initialize the centroids using the k-means++ algorithm, which selects the centroids in a more deterministic way.
- Once the K-means algorithm has converged, the data points will be assigned to one of the K clusters, and the centroids of the clusters will be the means of the data points in the cluster. These clusters and centroids can then be used to make predictions for new data points, by assigning them to the cluster with the nearest centroid.

### Expectation maximization:
- Expectation maximization (EM) is an iterative algorithm that is used to find the maximum likelihood estimate of the parameters of a model, given incomplete or noisy data. In the context of clustering, EM can be used to find the maximum likelihood estimate of the parameters of a mixture model, which is a probabilistic model that represents the data as a mixture of different distributions.
- The EM algorithm works by iteratively performing two steps: the expectation step (E-step) and the maximization step (M-step).
- In the E-step, the algorithm estimates the probability that each data point belongs to each of the clusters, given the current estimates of the parameters of the model.
- In the M-step, the algorithm estimates the parameters of the model that maximize the likelihood of the data, given the probabilities calculated in the E-step.
- The EM algorithm is particularly useful for clustering when the data is incomplete or noisy, because it allows the model to take into account the uncertainty in the data. It can also be used for clustering when the data is not well-separated, or when the clusters have different shapes or densities.
- The EM algorithm is an iterative algorithm, and it will continue to update the estimates of the parameters until convergence is reached, or until a maximum number of iterations has been reached. Once the EM algorithm has converged, the estimates of the parameters will represent the maximum likelihood estimate of the parameters of the mixture model. These estimates can then be used to make predictions for new data points by assigning them to the cluster with the highest probability.
- ADDITIONAL NOTES:
    - Derive a maximum likelihood estimation (infer parameters like mean, variance) for latent (unobserved) variables
    - https://youtube.com/playlist?list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt
    - Algorithm:
        - Start with randomly placed Gaussians
        - E-step: Find probability (Bayes' rule) each point belongs to each cluster (soft assignments, not hard assignments like K-means, i.e.- 0.75/0.25 intead of 1/0)
        - M-step: Adjust mean and variance for each cluster to fit points assigned to them
        - Iterate until convergence
    - ![Computing Probabilities](images/expectation_maximization.png)
    - Mostly generalizes to more dimensions, but need to look into covariance vs. variance
    - as with K-means, there are ways to deduce how many Gaussians you are fitting, but you need to penalize for larger numbers of Gaussians (the highest likelihood model is each data point has its own Gaussian)

### Gaussian mixture models:
- A Gaussian mixture model is a probabilistic model that represents a dataset as a mixture of multiple Gaussian distributions. It is a type of mixture model, which is a probabilistic model that represents the data as a mixture of different distributions.
- In the context of clustering, a Gaussian mixture model can be used to represent a dataset as a mixture of multiple clusters, where each cluster is represented by a Gaussian distribution. The parameters of the Gaussian distributions (such as the mean and covariance) can be estimated using the expectation maximization (EM) algorithm, which is an iterative algorithm that is used to find the maximum likelihood estimate of the parameters of a model given incomplete or noisy data.
- The advantage of using a Gaussian mixture model for clustering is that it allows the clusters to have a flexible shape, rather than being limited to spherical clusters like in K-means clustering. This can be useful when the clusters in the data are not well-separated, or when the clusters have different shapes or densities.
- To use a Gaussian mixture model for clustering, you must specify the number of clusters in the data. This can be done by selecting an appropriate value for the number of Gaussian distributions in the mixture model. Once the model has been fit to the data using the EM algorithm, the estimates of the parameters of the Gaussian distributions can be used to make predictions for new data points by assigning them to the cluster with the highest probability.
- ADDITIONAL NOTES:
    - https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95
    - https://youtu.be/DODphRRL79c
    - ![Gaussian Mixture](images/gaussian_mixture.png)
    - Think of this histogram as three separate Gaussians correpsonding to image classification clusters instead of an oddly shaped distribution
    - Weighted average of the categoric specific densities for the mixture density
    - The weights are pi, add up to 1
    - ![Gaussian Mixture Weights](images/gaussian_weights.png)
    - So each component Gaussian is represented by a mean, variance, and mixture component

## b. Generative models
- In machine learning, a generative model is a model that is used to generate new data samples that are similar to a training dataset. Generative models are often used for unsupervised learning, because they can learn the underlying structure of the data and use this knowledge to generate new data samples.
- There are several types of generative models, including:
    - Probabilistic generative models: These models represent the data as a probability distribution and use the principles of probability theory to generate new data samples. Examples of probabilistic generative models include Gaussian mixture models and latent Dirichlet allocation.
    - Deep generative models: These models use deep neural networks to learn the underlying structure of the data and generate new data samples. Examples of deep generative models include generative adversarial networks (GANs) and variational autoencoders (VAEs).
- Generative models can be used for a variety of purposes, such as:
    - Data augmentation: Generating new data samples can be used to increase the size of the training dataset, which can be useful for improving the performance of machine learning models.
    - Data visualization: Generative models can be used to visualize the structure of the data and understand the relationships between different features.
    - Data generation: Generative models can be used to generate new data samples that are similar to the training data, which can be useful for creating synthetic datasets or for generating realistic samples for testing or simulation.
- Overall, generative models are a powerful tool for unsupervised learning, because they can learn the underlying structure of the data and use this knowledge to generate new data samples.
- ADDITIONAL NOTES:
    - https://developers.google.com/machine-learning/gan/generative
    - Generative models calculate P(X, Y) or P(X), rather than discriminate models which capture P(Y | X)
    - Example: Rolling 3 six-sided dice and multiplying by a constant to model IQs in a population
    - Generative classifiers include: naive Bayes, linear discriminant analysis
    - More examples: https://en.wikipedia.org/wiki/Generative_model#Examples

## c. Representation Learning
- Representation learning is a type of unsupervised learning that is focused on learning useful representations of the data, rather than predicting a target variable or estimating the probability distribution of the data. The goal of representation learning is to find a compact and meaningful representation of the data that can be used for downstream tasks such as classification, clustering, or data visualization.
- There are several approaches to representation learning, including:
    - Autoencoders: These are neural network models that are trained to reconstruct the input data by learning a compressed representation of the data and then reconstructing the original data from this representation.
    - Generative models: These are models that are used to generate new data samples that are similar to a training dataset. Examples of generative models include Gaussian mixture models and deep generative models such as GANs and VAEs.
    - Transfer learning: This is a type of representation learning that involves pre-training a model on a large dataset and then fine-tuning it on a smaller dataset for a specific task. The pre-trained model is used to learn a good representation of the data that can be used for the specific task.
- Representation learning is an important area of unsupervised learning because it allows the machine learning model to learn the underlying structure of the data and find a compact and meaningful representation of the data that can be used for downstream tasks. It can also be used to improve the performance of machine learning models by providing a better representation of the data as input.

## d. Feature selection/transformation 
- In unsupervised learning, feature selection and transformation refers to the process of selecting and transforming the features in the data to improve the performance of a machine learning model. Feature selection involves selecting a subset of the features in the data to use as input to the model, while feature transformation involves transforming the features in some way to improve their representation or make them more suitable for the model.
- There are several reasons why feature selection and transformation can be useful in unsupervised learning:
    - Reducing the number of features in the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - Transforming the features in the data can help to improve their representation and make them more suitable for the model. For example, scaling the features can help to ensure that they are on the same scale, which can be important for certain types of models.
    - Transforming the features in the data can help to remove redundancy or noise from the data, which can improve the performance of the model.
- There are several approaches to feature selection and transformation in unsupervised learning, including:
    - Dimensionality reduction techniques: These techniques are used to reduce the number of features in the data by projecting the data onto a lower-dimensional space. Examples of dimensionality reduction techniques include PCA, SVD, and ICA.
    - Feature extraction techniques: These techniques are used to extract new features from the data that are more meaningful or informative for the model. Examples of feature extraction techniques include linear and non-linear projection methods, such as kernel PCA and autoencoders.
- Overall, feature selection and transformation can be an important part of unsupervised learning because they can help to improve the performance of the model by selecting and transforming the features in the data in a way that is more suitable for the model.




