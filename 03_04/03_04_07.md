# Data Preprocessing - Data Cleaning

## a. Missing values
- Missing values are values that are not present in a dataset. They can occur for a variety of reasons, such as data being lost or corrupted, or because a value was not collected for a particular sample.
- Dealing with missing values is an important step in the data cleaning process, because many machine learning algorithms cannot handle missing values and will fail or produce inaccurate results if they are present in the data.
- There are several approaches that can be used to handle missing values, depending on the characteristics of the data and the requirements of the machine learning algorithm. Some common approaches include:
    - Removing samples or features with missing values: This can be a simple way to handle missing values, but it can also result in the loss of a significant amount of data if a large number of samples or features have missing values.
    - Imputing missing values: This involves replacing the missing values with estimates based on the other values in the dataset. There are several methods that can be used to impute missing values, such as mean imputation, median imputation, and multiple imputation.    
    - Using algorithms that can handle missing values: Some machine learning algorithms, such as decision trees and random forests, can handle missing values internally and do not require the missing values to be imputed.
- The appropriate approach to handling missing values will depend on the characteristics of the data and the requirements of the machine learning algorithm.

## b. Outliers 
- Outliers are observations in a dataset that are significantly different from the majority of the data. They can occur due to a variety of reasons, such as errors in data collection or measurement, or because they represent rare or unusual events.
- Dealing with outliers is an important step in the data cleaning process, because they can have a significant impact on the results of a machine learning model. Outliers can cause problems for machine learning algorithms in several ways:
    - Outliers can influence the estimates of model parameters, causing the model to fit the outlier rather than the majority of the data.
    - Outliers can influence the predictions made by the model, causing the model to make extreme or inaccurate predictions for samples that are similar to the outlier.
    - Outliers can increase the variance of the model, causing the model to be more sensitive to the data and less robust.
- There are several approaches that can be used to identify and handle outliers in a dataset, depending on the characteristics of the data and the requirements of the machine learning algorithm. Some common approaches include:
    - Removing outliers: This involves identifying and removing the outlier observations from the dataset. This can be a simple way to handle outliers, but it can also result in the loss of important information if the outliers represent valid observations.
    - Transforming the data: This involves applying a transformation to the data to reduce the influence of the outliers. For example, applying a log transformation can reduce the influence of extreme values in the data.
    - Using algorithms that are resistant to outliers: Some machine learning algorithms, such as robust regression and decision trees, are less sensitive to the presence of outliers in the data.
- The appropriate approach to handling outliers will depend on the characteristics of the data and the requirements of the machine learning algorithm.
- NOTES 
    - Can randomly occur, but often are a result of measurement error.
    - For large sample sizes, outliers are expected and do not necessarily need to be excluded.
    - It is less frowned upon to discard outliers when there is confidence in the data following a normal distribution.
    - Truncation/trimming is an option, and Winsorring replaces outliers with the nearest non-suspect data.
    - Usually a z-score of +/- 3 can be considered an outlier.
    - Any data removal should be annotated.

## c. Feature enrichment 
- Feature enrichment is the process of adding additional features to a dataset in order to improve the performance of a machine learning model. It is a technique that is often used in the data cleaning process in order to improve the quality and predictive power of the data.
- There are several reasons why feature enrichment can be useful in machine learning:
    - Adding additional features can provide the model with more information about the samples, which can improve the model's ability to make accurate predictions.
    - Adding relevant features can help to reduce the bias of the model, because the model will have more information to base its predictions on.
    - Adding features that are correlated with the target variable can help to increase the variance of the model, which can lead to improved performance.
- There are several approaches that can be used to enrich the features in a dataset, depending on the characteristics of the data and the requirements of the machine learning algorithm. Some common approaches include:
    - Deriving features from existing features: This involves creating new features from combinations of existing features, or by applying transformations to existing features.
    - Adding external data: This involves incorporating additional data from external sources, such as external databases or publicly available data.
    - Adding domain-specific knowledge: This involves incorporating domain-specific knowledge or expert insights into the features of the dataset.
- The appropriate approach to feature enrichment will depend on the characteristics of the data and the requirements of the machine learning algorithm.
- NOTES
    - Example: edge detection in images.
    - Important when you have lesser amounts of data, less robust algorithms.

## d. Transformations 
- Transforming the data is a technique that is often used in the data cleaning process in order to improve the quality and predictive power of the data for a machine learning model. The purpose of data transformation is to change the scale, distribution, or other properties of the data in a way that is more suitable for the model.
- There are several reasons why data transformation can be useful in machine learning:
    - Transforming the data can help to improve the model's ability to learn from the data by making the data more amenable to the assumptions of the model.
    - Transforming the data can help to reduce the impact of noise or outliers in the data by smoothing the data or reducing the influence of extreme values.
    - Transforming the data can help to improve the interpretability of the model by making the relationship between the features and the target variable more interpretable.
- There are several types of data transformations that can be used, depending on the characteristics of the data and the requirements of the machine learning algorithm. Some common types of data transformations include:
    - Scaling: This involves changing the scale of the features in the data, such as by centering and normalizing the data.
    - Normalization: This involves transforming the data so that it has a Gaussian distribution with a mean of 0 and a standard deviation of 1.
    - Log transformation: This involves applying a logarithmic transformation to the data, which can be useful for reducing the influence of outliers or for making patterns in the data more interpretable.
- The appropriate data transformation will depend on the characteristics of the data and the requirements of the machine learning algorithm.
- NOTES
    - Can also include things like character set conversion, encoding handling, etc..