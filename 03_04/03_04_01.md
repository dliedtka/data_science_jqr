# 3.4.1 Data Preprocessing - Normalization/Rescaling

- Normalization, also known as scaling or rescaling, is a preprocessing step that is commonly used in machine learning. The purpose of normalization is to adjust the values of the features in the data so that they have similar scales and are in the same range.
- There are several reasons why normalization is useful in machine learning:
    - Some machine learning algorithms are sensitive to the scale of the input features, and can perform poorly if the features are not on a similar scale. For example, algorithms that use Euclidean distance as a measure of similarity, such as k-means clustering, are sensitive to the scale of the features.
    - Normalization can help to reduce the time required to train a model, because the optimization algorithm may converge more quickly when the features are on a similar scale.
    - Normalization can help to improve the interpretability of the model, because the coefficients of the model will be on a similar scale and can be more easily compared.
- There are several different methods that can be used to normalize the features in a dataset, such as min-max scaling, standardization, and mean normalization. The appropriate method to use depends on the characteristics of the data and the requirements of the machine learning algorithm.