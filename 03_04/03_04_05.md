# 3.4.5 Data Preprocessing - Dimensionality Reduction

Going to need to watch videos/read how to do all of these.

## a. Principal Component Analysis (PCA)
- Principal Component Analysis (PCA) is a dimensionality reduction technique that is used to reduce the complexity of a dataset by projecting the data onto a lower-dimensional space. It is a linear method that is based on the idea of finding the directions in the data that have the highest variance, and projecting the data onto these directions.
- There are several reasons why PCA can be useful as a dimensionality reduction technique:
    - Reducing the dimensionality of the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - Reducing the dimensionality of the data can help to remove redundancy and noise from the data, which can improve the performance of machine learning models.
    - Reducing the dimensionality of the data can help to visualize the data and understand the relationships between the features.
- To use PCA as a dimensionality reduction technique, the following steps can be followed:
    1. Standardize the data by centering and scaling the features.
    2. Compute the covariance matrix of the data.
    3. Compute the eigenvectors and eigenvalues of the covariance matrix.
    4. Select the top k eigenvectors, where k is the desired dimensionality of the output data.
    5. Transform the data by projecting it onto the space spanned by the top k eigenvectors.
- PCA is a powerful dimensionality reduction technique that can be used to reduce the complexity of a dataset and improve the performance of machine learning models. It is particularly useful when the data has a large number of features and the features are correlated with each other.
- ADDITIONAL NOTES:
    - A statistical technique to reduce the dimensionality of data.
    - "Increase the interpretability of a dataset while preserving the maximum amount of information." (Think about being able to plot data with dozens of features on a 2D graph.)

## b. Random Projections
- Random projections is a dimensionality reduction technique that is used to reduce the complexity of a dataset by projecting the data onto a lower-dimensional space using a random matrix. It is a non-linear method that is based on the idea of preserving the pairwise distances between the data points in the original space.
- There are several reasons why random projections can be useful as a dimensionality reduction technique:
    - Reducing the dimensionality of the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - Random projections can be faster and more computationally efficient than other dimensionality reduction techniques, such as singular value decomposition (SVD) or eigendecomposition, because it does not require the computation of the covariance matrix or the eigenvectors and eigenvalues of the matrix.
    - Random projections can be used to reduce the dimensionality of high-dimensional data while preserving the pairwise distances between the data points, which can be useful for certain types of machine learning algorithms that rely on pairwise distances, such as nearest neighbor algorithms.
- To use random projections as a dimensionality reduction technique, the following steps can be followed:
    1. Select the desired dimensionality of the output data, k.
    2. Generate a random matrix of size n x k, where n is the number of features in the original data.
    3. Transform the data by projecting it onto the space spanned by the random matrix.
- Random projections is a simple and efficient dimensionality reduction technique that can be used to reduce the complexity of a dataset and improve the performance of machine learning models. It is particularly useful when the data has a high number of features and computational efficiency is a concern.
- ADDITIONAL NOTES:
    - Reduce the dimensionality of a set of points which lie in a Euclidean space.
    - Simple and computationally efficient, trades a controlled amount of error for faster processing and smaller models.

## c. Independent Component Analysis (ICA)
- Independent Component Analysis (ICA) is a dimensionality reduction technique that is used to reduce the complexity of a dataset by separating the independent sources of variation in the data. It is a non-linear method that is based on the idea of finding the directions in the data that are maximally independent, and projecting the data onto these directions.
- There are several reasons why ICA can be useful as a dimensionality reduction technique:
    - Reducing the dimensionality of the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - ICA can be used to separate the independent sources of variation in the data, which can be useful for understanding the relationships between the features and for identifying the underlying structure of the data.
    - ICA can be used to denoise the data by removing redundant or correlated features from the data, which can improve the performance of machine learning models.
- To use ICA as a dimensionality reduction technique, the following steps can be followed:
    1. Standardize the data by centering and scaling the features.
    2. Compute the covariance matrix of the data.
    3. Decorrelate the data by applying a whitening transformation to the data.
    4. Find the independent components of the data by applying an ICA algorithm, such as FastICA or JADE.
    5. Select the top k independent components, where k is the desired dimensionality of the output data.
- ICA is a powerful dimensionality reduction technique that can be used to reduce the complexity of a dataset and improve the performance of machine learning models. It is particularly useful when the data has a large number of correlated features and the goal is to identify the independent sources of variation in the data.
- ADDITIONAL NOTES:
    - Separate multivariate data into additive subcomponents.
    - At most, one subcomponent is Gaussian and subcomponents are statistically independt of each other.

## d. Singular Value Decomposition (SVD)
- Singular Value Decomposition (SVD) is a dimensionality reduction technique that is used to reduce the complexity of a dataset by projecting the data onto a lower-dimensional space. It is a linear method that is based on the idea of decomposing the data matrix into the product of three matrices: a left singular matrix, a singular value matrix, and a right singular matrix.
- There are several reasons why SVD can be useful as a dimensionality reduction technique:
    - Reducing the dimensionality of the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - SVD can be used to decompose the data matrix into its underlying factors, which can be useful for understanding the relationships between the features and for identifying the underlying structure of the data.
    - SVD can be used to denoise the data by removing redundant or correlated features from the data, which can improve the performance of machine learning models.
- To use SVD as a dimensionality reduction technique, the following steps can be followed:
    1. Compute the SVD of the data matrix.
    2. Select the top k singular values and corresponding singular vectors, where k is the desired dimensionality of the output data.
    3. Transform the data by projecting it onto the space spanned by the top k singular vectors.
- SVD is a powerful dimensionality reduction technique that can be used to reduce the complexity of a dataset and improve the performance of machine learning models. It is particularly useful when the data has a large number of correlated features and the goal is to decompose the data matrix into its underlying factors.
- ADDITIONAL NOTES:
    - A factorization of a real or complex matrix.
    - I think this allows for more efficient storage of data

## e. Linear Discriminant Analysis (LDA)
- Linear Discriminant Analysis (LDA) is a dimensionality reduction technique that is used to reduce the complexity of a dataset by projecting the data onto a lower-dimensional space. It is a supervised method that is based on the idea of finding the directions in the data that maximize the separation between the different classes, and projecting the data onto these directions.
- LDA is typically used for classification tasks, where the goal is to predict the class label of a new data point based on a training dataset. It is particularly useful when the data has a large number of features and the classes are well-separated.
- There are several reasons why LDA can be useful as a dimensionality reduction technique:
    - Reducing the dimensionality of the data can make it more tractable and easier to work with, especially when the data has a high number of features.
    - LDA can be used to find the directions in the data that maximize the separation between the different classes, which can improve the performance of a classifier.
    - LDA can be used to visualize the data and understand the relationships between the features and the class labels.
- To use LDA as a dimensionality reduction technique, the following steps can be followed:
    1. Compute the mean and covariance matrix of the features for each class.
    2. Compute the within-class and between-class scatter matrices.
    3. Compute the eigenvectors and eigenvalues of the scatter matrices.
    4. Select the top k eigenvectors, where k is the desired dimensionality of the output data.
    5. Transform the data by projecting it onto the space spanned by the top k eigenvectors.
- LDA is a powerful dimensionality reduction technique that can be used to reduce the complexity of a dataset and improve the performance of a classifier. It is particularly useful when the data has a large number of features and the classes are well-separated.
