# Supervised Learning

### a. Regression vs. Classification
- regression predicts a continuous value (e.g.- price, score)
- classification predicts a discrete value (e.g.- win/loss, type of animal, gender)

### b. Linear/Logistic Regression
- linear regression is a regression method
    - in a single feature example, optimize m and b in y=mx+b
    - mean squared error function to minimize: $E = 1/n * \sum^n_{i=0} (y_i - (m x_i + b))^2$
    - $\partial E/\partial m = 1/n \sum^n_{i=0} (2 * (y_i - (m x_i + b)) * (-x_i)) = -2/n \sum^n_{i=0} x_i (y_i - (m x_i + b))$ 
    - $\partial E/\partial b = 1/n \sum^n_{i=0} (2 * (y_i - (m x_i + b)) * (-1)) = -2/n \sum^n_{i=0} (y_i - (m x_i + b))$
    - update function: $m = m - \alpha * \partial E/ \partial m$, $b = b - \alpha * \partial E/ \partial b$
- logistic regression is a classification method (not regression) 
    - $W$ is a 1-D vector, length is the number of features (n x 1)
    - $b$ is a constant
    - $X$ is (n x m), $Y$ is (1 x m)
    - sigmoid function: $\sigma = 1 / (1 + e^{-x})$
    - predictions (1 x m): $A = \sigma (W^T X + b)$
    - cost function: $c = - 1/m \sum^m_{i=1} [ y_i * \log(a_i) + (1 - y_i) * \log(1 - a_i) ]$
    - gradient descent: 
        - $dW = \partial c/\partial W = (A - Y) * X^T$ (1 x n)
        - $db = \partial c/\partial b = (A - Y)$
        - $W = W - \alpha dW^T$
        - $b = b - \alpha db$