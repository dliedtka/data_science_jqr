a. Curse of dimensionality
- As the number of features grows, the number of total samples we need to have several observations of every combination of features grows exponentially.

b. No free lunch theorem
- Any two optimization algorithms are equivalent when their performance is averaged over all possible problems.

c. Bias and variance
- https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229
- Bias is the difference between average prediction of our model and the correct value we are trying to predict. High bias is an oversimplified model and will perform poorly on training and test data.
- Variance is the variability of prediction for a given data point. High variance (overfitting) will lead to good performance on training data but poor performance on test data.

d. Probably approximately correct (PAC) learning
- When selecting a learner function for a task, the goal is for their to be a high probability of low generalization error.

e. Vapnik-Chervonenkis (VC) dimension 
- A measure of the "coverage" of an algorithm (How many data points can it reach? Can it learn a perfect classifier for a problem?)
